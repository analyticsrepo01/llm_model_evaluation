{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE-v-z7XqG4R"
   },
   "source": [
    "#**Vertex Evaluation For LLM - Demo**\n",
    "\n",
    "**Authors**: jsndai@\n",
    "\n",
    "This notebook showcases how to launch a Kubeflow pipeline (KFP) using the [`LLM Evaluation Component`](google3/third_party/py/vertexevaluation/llm/component/eval_component.py) for Generative Language Models on Vertex AI Managed Pipelines.\n",
    "\n",
    "***Please make a copy of this notebook to execute your own pipelines.***\n",
    "\n",
    "Terms of Service: This content is experimental functionality covered by the Pre-GA Offerings Terms of your Google Cloud Platform [Terms of Service](https://cloud.google.com/terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4lLQ5OAKfva"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "# ***Please create a COPY of this colab before running.***\n",
    "\n",
    "\n",
    "1. Please follow the\n",
    "- `Setup`,\n",
    "- `Configure your GCP project`,\n",
    "- `Test Vertex SDK for LLM Evaluation`\n",
    "- `Test LLM Evaluation Pipeline` sections below.\n",
    "\n",
    "2. Update the pipeline parameters in the `Define the Inputs Specific to the pipeline` section if you would like to customize the evaluation job.\n",
    "\n",
    "3. If any bugs arise or a pipeline fails, please file a ticket [here](https://b.corp.google.com/issues/new?component=865810&template=1816845)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ql0Nw-Wb3T6"
   },
   "source": [
    "#  1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73LBND5bQ66m"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "### To use the current version of LLM Eval in the Vertex SDK, you'll need:\n",
    "\n",
    "* A tuned LLM Model in prod environment. This notebook uses one in `pyc-llm-dev` GCP project, you can use one in your own project if you'd like.\n",
    "\n",
    "* Read access to the `gs://vertex_sdk_private_releases/` bucket (you should already have access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPH-fFKisH8_"
   },
   "source": [
    "## Authenticate your GCP account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0OJaxbZRsKjD"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth as google_auth\n",
    "  google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGow-hL-rnNB"
   },
   "source": [
    "## Install dependencies\n",
    "\n",
    "Please make sure to click on the \"RESTART RUNTIME\" button in the output after pip install completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHzgrAfWroUf",
    "outputId": "598fb1c2-baec-41af-83ce-2dbaf3a29480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the Google Cloud Pipeline Components (GCPC) & Vertex SDK.\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    USER = \"--user\"\n",
    "else:\n",
    "    USER = \"\"\n",
    "!pip3 install {USER} --upgrade google-cloud-aiplatform -q --no-warn-conflicts\n",
    "!pip3 install {USER} --upgrade google-cloud-pipeline-components -q --no-warn-conflicts\n",
    "!pip3 install {USER} --upgrade kfp -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHMfXkKpPg7W",
    "outputId": "ad437196-c460-431f-be98-2e190d57642a"
   },
   "outputs": [],
   "source": [
    "# !gsutil cp gs://vertex_sdk_private_releases/sara_test/google_cloud_aiplatform-1.26.dev20230530+language.models.eval-py2.py3-none-any.whl .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IcfaLAzOnTbw",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing ./google_cloud_aiplatform-1.26.dev20230530+language.models.eval-py2.py3-none-any.whl\n",
      "Collecting shapely<2.0.0\n",
      "  Using cached Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 from https://files.pythonhosted.org/packages/c4/1e/924dcad4725d2e697888e044edf7a433db84bf9a3e40d3efa38ba859d0ce/google_api_core-2.14.0-py3-none-any.whl.metadata\n",
      "  Using cached google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0 (from google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for proto-plus<2.0.0dev,>=1.22.0 from https://files.pythonhosted.org/packages/36/5b/e02636d221917d6fa2a61289b3f16002eb4c93d51c0191ac8e896d527182/proto_plus-1.22.3-py3-none-any.whl.metadata\n",
      "  Using cached proto_plus-1.22.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 from https://files.pythonhosted.org/packages/ae/5b/7ed02a9b8e752c8f7bca8661779c0275b9e3e6a903a3045e6da51f796dda/protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting packaging>=14.3 (from google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for packaging>=14.3 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for google-cloud-storage<3.0.0dev,>=1.32.0 from https://files.pythonhosted.org/packages/04/72/71b1b531cefa1daff8f6a2a70b4d4fa18dd4da851b5486d53578811b0838/google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for google-cloud-bigquery<4.0.0dev,>=1.15.0 from https://files.pythonhosted.org/packages/51/8c/bf168c5450431734d67ed4db3e62e2c81fbf2c7d8c0ff3153808e9ab480f/google_cloud_bigquery-3.13.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_bigquery-3.13.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for google-cloud-resource-manager<3.0.0dev,>=1.3.3 from https://files.pythonhosted.org/packages/8b/9c/6807473e69fddc9bf33413b7db966fbcfeb0deade2f5ed324cef2b98ec16/google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0.dev0,>=1.56.2 from https://files.pythonhosted.org/packages/21/49/12996dc0238e017504dceea1d121a48bd49fb3f4416f40d59fc3e924b4f3/googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for google-auth<3.0.dev0,>=2.14.1 from https://files.pythonhosted.org/packages/86/a7/75911c13a242735d5aeaca6a272da380335ff4ba5f26d6b2ae20ff682d13/google_auth-2.23.4-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting requests<3.0.0.dev0,>=2.18.0 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for requests<3.0.0.dev0,>=2.18.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for grpcio<2.0dev,>=1.33.2 from https://files.pythonhosted.org/packages/d8/d0/0c42b56820f399f9bbcb4441fba1d4e52af3f11fa51c40c553fbd404aa1a/grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for grpcio-status<2.0.dev0,>=1.33.2 from https://files.pythonhosted.org/packages/0f/31/9f87b4d6a5a03c92bab47d54bf516b7196667441e86550280178714bdb28/grpcio_status-1.59.3-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.59.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for google-cloud-core<3.0.0dev,>=1.6.0 from https://files.pythonhosted.org/packages/a2/40/02045f776fdb6e44194f34b6375a26ce8a61bd9bd03cd8930ed91cf51a62/google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for google-resumable-media<3.0dev,>=0.6.0 from https://files.pythonhosted.org/packages/c7/4f/b8e5e22406e5aeafa46df8799939d5eeee52f18eeed339675167fac6603a/google_resumable_media-2.6.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_resumable_media-2.6.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for grpc-google-iam-v1<1.0.0dev,>=0.12.4 from https://files.pythonhosted.org/packages/5f/4b/404f59d065a410e835576433bc296599ae093460c7724fa5d5ca2354a885/grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl.metadata\n",
      "  Using cached grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Using cached google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/da/f1/3702ba2a7470666a62fd81c58a4c40be00670e5006a67f4d626e57f013ae/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/96/94/c31f58c7a7f470d5665935262ebd7455c7e4c7782eb525658d3dbf4b9403/urllib3-2.1.0-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/64/62/428ef076be88fa93716b576e4a01f919d25968913e817077a386fcbe4f42/certifi-2023.11.17-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.dev20230530+language.models.eval)\n",
      "  Obtaining dependency information for pyasn1<0.6.0,>=0.4.6 from https://files.pythonhosted.org/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached google_cloud_bigquery-3.13.0-py2.py3-none-any.whl (222 kB)\n",
      "Using cached google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl (320 kB)\n",
      "Using cached google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "Using cached protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached google_api_core-2.14.0-py3-none-any.whl (122 kB)\n",
      "Using cached google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "Using cached googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "Using cached grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl (26 kB)\n",
      "Using cached grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Using cached grpcio_status-1.59.3-py3-none-any.whl (14 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: urllib3, six, shapely, pyasn1, protobuf, packaging, idna, grpcio, google-crc32c, charset-normalizer, certifi, cachetools, rsa, requests, python-dateutil, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, grpc-google-iam-v1, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: shapely\n",
      "    Found existing installation: Shapely 1.8.5.post1\n",
      "    Uninstalling Shapely-1.8.5.post1:\n",
      "      Successfully uninstalled Shapely-1.8.5.post1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.1\n",
      "    Uninstalling pyasn1-0.5.1:\n",
      "      Successfully uninstalled pyasn1-0.5.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.59.3\n",
      "    Uninstalling grpcio-1.59.3:\n",
      "      Successfully uninstalled grpcio-1.59.3\n",
      "  Attempting uninstall: google-crc32c\n",
      "    Found existing installation: google-crc32c 1.5.0\n",
      "    Uninstalling google-crc32c-1.5.0:\n",
      "      Successfully uninstalled google-crc32c-1.5.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.2\n",
      "    Uninstalling cachetools-5.3.2:\n",
      "      Successfully uninstalled cachetools-5.3.2\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1-modules 0.3.0\n",
      "    Uninstalling pyasn1-modules-0.3.0:\n",
      "      Successfully uninstalled pyasn1-modules-0.3.0\n",
      "  Attempting uninstall: proto-plus\n",
      "    Found existing installation: proto-plus 1.22.3\n",
      "    Uninstalling proto-plus-1.22.3:\n",
      "      Successfully uninstalled proto-plus-1.22.3\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.61.0\n",
      "    Uninstalling googleapis-common-protos-1.61.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.61.0\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 2.6.0\n",
      "    Uninstalling google-resumable-media-2.6.0:\n",
      "      Successfully uninstalled google-resumable-media-2.6.0\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.48.2\n",
      "    Uninstalling grpcio-status-1.48.2:\n",
      "      Successfully uninstalled grpcio-status-1.48.2\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.23.4\n",
      "    Uninstalling google-auth-2.23.4:\n",
      "      Successfully uninstalled google-auth-2.23.4\n",
      "  Attempting uninstall: grpc-google-iam-v1\n",
      "    Found existing installation: grpc-google-iam-v1 0.12.7\n",
      "    Uninstalling grpc-google-iam-v1-0.12.7:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.12.7\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.14.0\n",
      "    Uninstalling google-api-core-2.14.0:\n",
      "      Successfully uninstalled google-api-core-2.14.0\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.3.3\n",
      "    Uninstalling google-cloud-core-2.3.3:\n",
      "      Successfully uninstalled google-cloud-core-2.3.3\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.13.0\n",
      "    Uninstalling google-cloud-storage-2.13.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.13.0\n",
      "  Attempting uninstall: google-cloud-resource-manager\n",
      "    Found existing installation: google-cloud-resource-manager 1.10.4\n",
      "    Uninstalling google-cloud-resource-manager-1.10.4:\n",
      "      Successfully uninstalled google-cloud-resource-manager-1.10.4\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.13.0\n",
      "    Uninstalling google-cloud-bigquery-3.13.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.13.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.36.4\n",
      "    Uninstalling google-cloud-aiplatform-1.36.4:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.36.4\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.4.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\n",
      "kfp 2.4.0 requires urllib3<2.0.0, but you have urllib3 2.1.0 which is incompatible.\n",
      "gcsfs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2023.10.0 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.14.0 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.1 which is incompatible.\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\n",
      "llama-index 0.8.58 requires urllib3<2, but you have urllib3 2.1.0 which is incompatible.\n",
      "llava 1.1.3 requires scikit-learn==1.2.2, but you have scikit-learn 1.3.1 which is incompatible.\n",
      "llava 1.1.3 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\n",
      "vllm 0.2.2 requires transformers>=4.34.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 google-api-core-2.14.0 google-auth-2.23.4 google-cloud-aiplatform-1.26.dev20230530+language.models.eval google-cloud-bigquery-3.13.0 google-cloud-core-2.3.3 google-cloud-resource-manager-1.10.4 google-cloud-storage-2.13.0 google-crc32c-1.5.0 google-resumable-media-2.6.0 googleapis-common-protos-1.61.0 grpc-google-iam-v1-0.12.7 grpcio-1.59.3 grpcio-status-1.59.3 idna-3.6 packaging-23.2 proto-plus-1.22.3 protobuf-4.25.1 pyasn1-0.5.1 pyasn1-modules-0.3.0 python-dateutil-2.8.2 requests-2.31.0 rsa-4.9 shapely-1.8.5.post1 six-1.16.0 urllib3-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Installing the SDK from a whl file\n",
    "# !gsutil cp gs://vertex_sdk_private_releases/sara_test/google_cloud_aiplatform-1.26.dev20230530+language.models.eval-py2.py3-none-any.whl .\n",
    "\n",
    "!pip install google_cloud_aiplatform-1.26.dev20230530+language.models.eval-py2.py3-none-any.whl \"shapely<2.0.0\" --force-reinstall --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwCyxycmQbmP"
   },
   "source": [
    "### Restart the kernel runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rd0EbrpQQbMM",
    "outputId": "8b1d7fa4-06d3-42b7-bd45-8f6a260ee89d"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI2qthrwQj9u"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install etils\n",
    "# !pip install kfp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import aiplatform\n",
    "# print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mName: google-cloud-aiplatform\n",
      "Version: 1.26.dev20230530+language.models.eval\n",
      "Summary: Vertex AI API client library\n",
      "Home-page: https://github.com/googleapis/python-aiplatform\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: /home/jupyter/.local/lib/python3.10/site-packages\n",
      "Requires: google-api-core, google-cloud-bigquery, google-cloud-resource-manager, google-cloud-storage, packaging, proto-plus, protobuf, shapely\n",
      "Required-by: google-cloud-pipeline-components\n"
     ]
    }
   ],
   "source": [
    "!pip show google.cloud.aiplatform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NfjGzcLZr9uI"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "# import kfp\n",
    "# from etils import epath\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import sys; sys.path.append('/opt/conda/envs/pytorch/lib/python3.10/site-packages/')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8df7Uvv-oLUu"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import language_models\n",
    "\n",
    "# from vertexai.preview import language_models_eval\n",
    "\n",
    "# import google.cloud.aiplatform #private_preview import language_models_eval #_evaluatable_language_models\n",
    "\n",
    "from google.cloud.aiplatform.private_preview.language_models_eval import _evaluatable_language_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXGHg83osBod"
   },
   "source": [
    "# 2. Configure your GCP project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X213_vaARAmD"
   },
   "source": [
    "## Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "i461wkwPREVI"
   },
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"pyc-llm-dev\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "VERTEX_API_PROJECT = PROJECT_ID = \"my-project-0004-346516\" #'your-project' #@param {\"type\": \"string\"}\n",
    "# REGION = LOCATION = \"europe-west4\" # 'us-central1' #\"europe-west4\"\n",
    "GCS_BUCKET = STAGING_BUCKET = DATA_STAGING_GCS_LOCATION = 'gs://my-project-0004-346516' #\"my-project-0004-346516-vertex-pipelines-europe-west4\"\n",
    "\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1HRrh6Mmj4C",
    "outputId": "2254664d-6a69-420b-8024-d13738b09b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJgYsIGQR6J5"
   },
   "source": [
    "## Configure a test GCS bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "8dGDyf0dR4qU"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"my-project-0004-346516\" # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# bucket_path = epath.Path(BUCKET_URI)\n",
    "\n",
    "# SDK Configuration\n",
    "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b2CpckyowQY"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets, and output performance metrics file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHVxY2slSkzW"
   },
   "source": [
    "\n",
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JW5tbVsfSmf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://my-project-0004-346516/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'my-project-0004-346516' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJPA1qC1pLmI"
   },
   "source": [
    "## Configure GCS folder for running pipelines\n",
    "\n",
    "Evaluation related files (Eval Metrics, Batch Prediction results) will\n",
    "be saved to the GCS bucket. The pipeline will not clean up the files since\n",
    "some of them might be useful for you, please make sure to clean up them if\n",
    "needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "M7omAA_6sCx4",
    "outputId": "665c27e1-cd46-45ab-da17-7b1a44edcaff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://my-project-0004-346516/eval-fishfooding-pipelines'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcs_base_path = f\"{BUCKET_URI}/eval-fishfooding-pipelines\"\n",
    "gcs_base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63SXFt39UgaQ"
   },
   "source": [
    "# 3. Test LLM Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv9vBuH3ZY11"
   },
   "source": [
    "## Load compiled template pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "v2d_T-D9WiyP"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Define the function to read metrics content from GCS\n",
    "def get_metrics_blob(job, nlp_task):\n",
    "  expected_task_name = \"model-evaluation-text-generation\" if nlp_task != \"\" else \"model-evaluation-classification\"\n",
    "  task_detail = None\n",
    "  for detail in job.task_details:\n",
    "    if detail.task_name == expected_task_name:\n",
    "      task_detail = detail\n",
    "  if not task_detail:\n",
    "    print(f\"Not able to find the task {expected_task_name}.\")\n",
    "  metrics_uri = None\n",
    "  for k, v in task_detail.outputs.items():\n",
    "    if k != \"evaluation_metrics\":\n",
    "      continue\n",
    "    for artifact in v.artifacts:\n",
    "      if artifact.display_name == \"evaluation_metrics\":\n",
    "        metrics_uri = artifact.uri[5:]\n",
    "  if not metrics_uri:\n",
    "    print(\"Not able to find the metric.\")\n",
    "  splits = metrics_uri.split(\"/\")\n",
    "  bucket_name = splits[0]\n",
    "  blob_name = '/'.join(splits[1:])\n",
    "  bucket = storage_client.bucket(bucket_name)\n",
    "  blob = bucket.blob(blob_name)\n",
    "  with blob.open(\"r\") as f:\n",
    "    return json.loads(f.read())\n",
    "\n",
    "# Define the function to plot confusion matrix\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "def plot_confusion_matrix(job, nlp_task):\n",
    "  overall_metrics = get_metrics_blob(job, nlp_task)\n",
    "  confusion_matrix = []\n",
    "  for slice_metric in overall_metrics['slicedMetrics']:\n",
    "    if 'value' in slice_metric['singleOutputSlicingSpec']:\n",
    "      continue\n",
    "    if 'confusionMatrix' not in slice_metric['metrics']['classification']:\n",
    "      print(\"No Confusion Matrix found\")\n",
    "      print(f\"Evaluation metrics is: {slice_metric}\")\n",
    "      return\n",
    "    for row in slice_metric['metrics']['classification']['confusionMatrix']['rows']:\n",
    "      confusion_matrix.append(row['dataItemCounts'])\n",
    "  # Plot the matrix\n",
    "  confusion_matrix = numpy.array(confusion_matrix)\n",
    "\n",
    "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = evaluation_class_labels)\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(8,8))\n",
    "  cm_display.plot(ax=ax)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Define the function to print nlp metrics\n",
    "from tabulate import tabulate\n",
    "\n",
    "def print_nlp_metrics(job, nlp_task):\n",
    "  metrics = get_metrics_blob(job, nlp_task)\n",
    "  metric_names = []\n",
    "  if nlp_task == \"question-answering\":\n",
    "    metric_names = [\"exact_match\"]\n",
    "  elif nlp_task == \"summarization\":\n",
    "    metric_names = [\"rougeLSum\"]\n",
    "  else:\n",
    "    metric_names = [\"bleu\", \"rougeLsum\"]\n",
    "  table = [metric_names, [metrics[metric_name] for metric_name in metric_names]]\n",
    "  print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "\n",
    "# Define the function to print classification metrics\n",
    "def print_classification_metrics(job, nlp_task):\n",
    "  all_metrics = get_metrics_blob(job, nlp_task)['slicedMetrics']\n",
    "  overall_metrics = all_metrics[0]['metrics']['classification']\n",
    "  metric_names = [\"Metric Slice\", \"auPrc\", \"auRoc\", \"logLoss\"]\n",
    "  f1_metrics = [\"f1Score\"]\n",
    "  aggregated_f1_metrics = [\"f1ScoreMicro\", \"f1ScoreMacro\"]\n",
    "  table = [metric_names + f1_metrics + aggregated_f1_metrics]\n",
    "  for metrics in all_metrics:\n",
    "    classification_metric = metrics['metrics']['classification']\n",
    "    slice_name = \"class - \" + metrics['singleOutputSlicingSpec']['value'] if 'value' in metrics['singleOutputSlicingSpec'] else \"Overall\"\n",
    "    slice_metric_values = [slice_name]\n",
    "    slice_metric_values.extend([classification_metric.get(metric_name, 0) for metric_name in metric_names[1:]])\n",
    "    slice_metric_values.extend([classification_metric['confidenceMetrics'][0].get(metric_name, 0) for metric_name in f1_metrics])\n",
    "    slice_metric_values.extend([classification_metric['confidenceMetrics'][0].get(metric_name, 'n/a') for metric_name in aggregated_f1_metrics])\n",
    "    table.append(slice_metric_values)\n",
    "  print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "\n",
    "# Define the function to print confidence metrics\n",
    "def print_confidence_metrics(job, nlp_task, expected_confidence_threshold):\n",
    "  all_metrics = get_metrics_blob(job, nlp_task)['slicedMetrics']\n",
    "  confidence_metric_names = [\"Metric Slice\", \"recall\", \"precision\", \"falsePositiveRate\", \"f1Score\", \"truePositiveCount\", \"falsePositiveCount\"]\n",
    "  table = [confidence_metric_names]\n",
    "  for metrics in all_metrics:\n",
    "    classification_metric = metrics['metrics']['classification']\n",
    "    slice_name = \"class - \" + metrics['singleOutputSlicingSpec']['value'] if 'value' in metrics['singleOutputSlicingSpec'] else \"Overall\"\n",
    "    slice_metric_values = [slice_name]\n",
    "    confidence_metrics = None\n",
    "    found_threshold_distance = 1\n",
    "    for metrics in classification_metric['confidenceMetrics']:\n",
    "      confidence_threshold = metrics['confidenceThreshold'] if 'confidenceThreshold' in metrics else 0\n",
    "      if abs(expected_confidence_threshold-confidence_threshold) <= found_threshold_distance:\n",
    "        confidence_metrics = metrics\n",
    "        found_threshold_distance = abs(expected_confidence_threshold-confidence_threshold)\n",
    "    slice_metric_values.extend([confidence_metrics.get(metric_name, 0) for metric_name in confidence_metric_names[1:]])\n",
    "    table.append(slice_metric_values)\n",
    "  print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "\n",
    "\n",
    "\n",
    "evaluation_llm_text_generation_pipeline = \"https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation-llm-text-generation-pipeline/1.0.1\"#@param {type:\"string\"}\n",
    "evaluation_llm_classification_pipeline = \"https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation-llm-classification-pipeline/1.0.1\"#@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation-llm-text-generation-pipeline/1.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "q5uRvFINRwvN"
   },
   "outputs": [],
   "source": [
    "# Model in prod environment from `pyc-llm-dev` project.\n",
    "model_name = \"projects/255766800726/locations/us-central1/models/1231208931527753728\" #@param {type:\"string\"}\n",
    "model_name = \"projects/255766800726/locations/us-central1/models/998292786346196992\"\n",
    "\n",
    "# model_name = \"projects/255766800726/locations/us-central1/models/8377665076164820992\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "\n",
    "# \"publishers/google/models/text-bison@001\"\n",
    "# \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbNpFc8COvph"
   },
   "source": [
    "## Select your test dataset file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIzhhTObPEWG"
   },
   "source": [
    "Select a public test dataset for Fishfooding. You are also encouraged to use your own test dataset for testing.\n",
    "\n",
    "Task Type  | GCS URI\n",
    "------- | --------\n",
    "Text Generation | gs://vertex-evaluation-llm-dataset-us-central1/test_datasets/text_generation_bp_input_with_ground_truth.jsonl\n",
    "Text Classification | gs://vertex-evaluation-llm-dataset-us-central1/test_datasets/llm_classification_bp_input_prompts_with_ground_truth.jsonl |\n",
    "Question Answering | gs://vertex-evaluation-llm-dataset-us-central1/test_datasets/qa_bp_input.jsonl\n",
    "Summarization | gs://vertex-evaluation-llm-dataset-us-central1/test_datasets/summarization_bp_input.jsonl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "8Ncr5TCMQWFq"
   },
   "outputs": [],
   "source": [
    "# this is a common GCS and may not be accessible to service ID hence copy it to my own bucket first\n",
    "\n",
    "batch_predict_gcs_source_uris = 'gs://vertex-evaluation-llm-dataset-us-central1/test_datasets/text_generation_bp_input_with_ground_truth.jsonl'#@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://vertex-evaluation-llm-dataset-us-central1/test_datasets/text_generation_bp_input_with_ground_truth.jsonl [Content-Type= text/plain]...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp -r $batch_predict_gcs_source_uris gs://my-project-0004-346516/eval-fishfooding-pipelines\n",
    "\n",
    "# ! gsutil cp -r $batch_predict_gcs_source_uris gs://my-project-0004-346516my-project-0004-346516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_gcs_source_uris = 'gs://my-project-0004-346516/eval-fishfooding-pipelines/text_generation_bp_input_with_ground_truth.jsonl'#@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Kk1LQ3bpMeQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prompt\":\"Basketball teams in the Midwest.\", \"ground_truth\":\"There are several basketball teams located in the Midwest region of the United States. Here are some of them:\"}\n",
      "{\"prompt\":\"How to bake gluten-free bread?\", \"ground_truth\":\"Baking gluten-free bread can be a bit challenging because gluten is the protein that gives bread its structure and elasticity.\"}\n",
      "{\"prompt\":\"Want to buy a new phone.\", \"ground_truth\":\"Great! There are many factors to consider when buying a new phone, including your budget, preferred operating system, desired features, and more. Here are some general steps to follow to help you make an informed decision:\"}\n",
      "{\"prompt\":\"I told them \\\"see you tomorrow\\\"\", \"ground_truth\":\"If you told someone \\\"see you tomorrow,\\\" you most likely meant that you will see them the following day. This is a common phrase used when saying goodbye to someone with the intention of seeing them again soon. If you are unable to meet with them as planned, it is always polite to let them know as soon as possible so that they are not left waiting or wondering.\"}\n",
      "{\"prompt\":\"Apples are red\\nOranges are orange\", \"ground_truth\":\"Yes, apples are often red, although there are other varieties of apples that can be green, yellow, or a combination of these colors. Oranges are typically orange, but they can also be green or even slightly red depending on their variety and ripeness. Colors are an important characteristic of fruits, as they can be an indication of ripeness and nutritional value.\"}"
     ]
    }
   ],
   "source": [
    "# Peek at your BP input file.\n",
    "! gsutil cat $batch_predict_gcs_source_uris | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "A8M85PyAQpl7"
   },
   "outputs": [],
   "source": [
    "target_field_name='instance.ground_truth' #@param {type:\"string\"}\n",
    "prediction_field_name='predictions.content' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n-NqkU3tfv1"
   },
   "source": [
    "## [Option 1] Submit a Eval Pipeline for QA task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEEA1CJ0jqBh"
   },
   "source": [
    "### Define the Inputs Specific to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "_dDxl_J_joZV"
   },
   "outputs": [],
   "source": [
    "evaluation_task = nlp_task ='question-answering' #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieDb-XkXaEk_"
   },
   "source": [
    "### Submit a pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "POAOo_Q5th5X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760?project=255766800726\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-qa-9c54fb6c-ebaf-4bd0-96dd-fcbdee75d760\n"
     ]
    }
   ],
   "source": [
    "# We need to provide the parameter for all arguments that does not have a default value.\n",
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"evaluation_task\": evaluation_task,\n",
    "    \"batch_predict_gcs_source_uris\": [batch_predict_gcs_source_uris],\n",
    "    \"batch_predict_gcs_destination_output_uri\": gcs_base_path,\n",
    "    \"model_name\": model_name,\n",
    "}\n",
    "\n",
    "job_id = \"fishfood-llm-eval-test-QA-{}\".format(uuid.uuid4()).lower()\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    template_path=evaluation_llm_text_generation_pipeline,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=gcs_base_path,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDKCSNk93jPb"
   },
   "source": [
    "### View the QA task Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PcDd-JVt7ehf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   exact_match \n",
      "\n",
      "             0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_nlp_metrics(job, nlp_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_JQ5_J6aP5J"
   },
   "source": [
    "## [Option 2] Submit a Eval Pipeline for Summarization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qTES1OfaP5J"
   },
   "source": [
    "### Define the Inputs Specific to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "form",
    "id": "Vjg2ZD7oaP5K"
   },
   "outputs": [],
   "source": [
    "evaluation_task = 'summarization' #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HObWDVqaP5K"
   },
   "source": [
    "### Submit a pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HF6_iqZiaP5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40?project=255766800726\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/fishfood-llm-eval-test-summarization-4f35dcc0-74dc-4b50-971d-689ef2358c40\n"
     ]
    }
   ],
   "source": [
    "# We need to provide the parameter for all arguments that does not have a default value.\n",
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"evaluation_task\": evaluation_task,\n",
    "    \"batch_predict_gcs_source_uris\": [batch_predict_gcs_source_uris],\n",
    "    \"batch_predict_gcs_destination_output_uri\": gcs_base_path,\n",
    "    \"model_name\": model_name,\n",
    "\n",
    "}\n",
    "\n",
    "job_id = \"fishfood-llm-eval-test-summarization-{}\".format(uuid.uuid4()).lower()\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    template_path=evaluation_llm_text_generation_pipeline,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=gcs_base_path,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tmSpfSy3-9A"
   },
   "source": [
    "### View the Summarization task Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VHNHoUjZ7aha"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   rougeLSum \n",
      "\n",
      "   0.0888592 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_nlp_metrics(job, evaluation_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYENbCwRbnlu"
   },
   "source": [
    "## [Option 3] Submit a Eval Pipeline for General Text Generation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJRz0Iotbnlu"
   },
   "source": [
    "### Define the Inputs Specific to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_xml4V8bnlu"
   },
   "outputs": [],
   "source": [
    "evaluation_task = 'text-generation' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3holpDG9bnlu"
   },
   "source": [
    "### Submit a pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sk7iIS1-bnlu"
   },
   "outputs": [],
   "source": [
    "# We need to provide the parameter for all arguments that does not have a default value.\n",
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"evaluation_task\": evaluation_task,\n",
    "    \"batch_predict_gcs_source_uris\": [batch_predict_gcs_source_uris],\n",
    "    \"batch_predict_gcs_destination_output_uri\": gcs_base_path,\n",
    "    \"model_name\": model_name,\n",
    "}\n",
    "\n",
    "job_id = \"fishfood-llm-eval-test-summarization-{}\".format(uuid.uuid4()).lower()\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    template_path=evaluation_llm_text_generation_pipeline,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=gcs_base_path,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xeLyDDNeykY"
   },
   "source": [
    "### View the General Text Generation task Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGTz4shQ7vLN"
   },
   "outputs": [],
   "source": [
    "print_nlp_metrics(job, evaluation_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTdtIgR7Bkrn"
   },
   "source": [
    "## [Option 4] Submit a Eval Pipeline for Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DMi9ZcuBrOi"
   },
   "source": [
    "### Define the Inputs Specific to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn75VZEGBt3l"
   },
   "outputs": [],
   "source": [
    "target_field_name='ground_truth' #@param {type:\"string\"}\n",
    "evaluation_class_labels=['nature', 'news', 'sports', 'health', 'startups'] #@param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6y1i63UOJED"
   },
   "source": [
    "### Submit a pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50y34QUbByXo"
   },
   "outputs": [],
   "source": [
    "# We need to provide the parameter for all arguments that does not have a default value.\n",
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"batch_predict_gcs_destination_output_uri\": gcs_base_path,\n",
    "    \"evaluation_class_labels\": evaluation_class_labels,\n",
    "    \"batch_predict_gcs_source_uris\": [batch_predict_gcs_source_uris],\n",
    "    \"target_field_name\": target_field_name,\n",
    "    \"model_name\": model_name,\n",
    "}\n",
    "\n",
    "job_id = \"fishfood-llm-eval-test-classification-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    template_path=evaluation_llm_classification_pipeline,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=gcs_base_path,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Qg3Qp43_lyW"
   },
   "source": [
    "### View the Classification task Evaluation metrics for whole Dataset and each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64Ud3ctRFE4y"
   },
   "outputs": [],
   "source": [
    "print_classification_metrics(job, evaluation_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1NFOhA6_zDA"
   },
   "source": [
    "### Spot check Confusion Matrix for the whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "664fBwO7_yaT"
   },
   "outputs": [],
   "source": [
    "overall_metrics = plot_confusion_matrix(job, evaluation_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIJL_2UbMEKH"
   },
   "source": [
    "### Spot check Confidence Metrics for the whole Dataset and each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOdEzYjhGPGa"
   },
   "outputs": [],
   "source": [
    "print_confidence_metrics(job, nlp_task, expected_confidence_threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj75VpSepBwt"
   },
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5W7W07ppFSL"
   },
   "source": [
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVzTpU5BpC8h"
   },
   "outputs": [],
   "source": [
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QPGt4VQu4M2"
   },
   "source": [
    "# How It Works / FAQ\n",
    "To use evaluation on Vertex Managed Pipelines, there's a couple terms to get familiar with. ***(Check out the bolded parts in each section!)***\n",
    "### 1. Kubeflow Pipelines (KFP) vs Managed Pipelines (MP)\n",
    "Kubeflow is a machine learning toolkit that is dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable.\n",
    "\n",
    "[Kubeflow pipelines](https://github.com/kubeflow/pipelines) are reusable end-to-end ML workflows built using the Kubeflow Pipelines SDK.\n",
    "\n",
    "[Managed Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata.\n",
    "\n",
    "***In summary: Managed Pipelines used in this fishfooding is a hosted orchestration on the Vertex Platform of Kubeflow Pipelines.***\n",
    "\n",
    "### 2. Pipeline Components\n",
    "Pipeline components are self-contained sets of code that perform one part of a pipeline's workflow, such as data preprocessing, data transformation, and training a model.\n",
    "\n",
    "Components are composed of a set of inputs, a set of outputs, and the location of a container image. A component's container image is a package that includes the component's executable code and a definition of the environment that the code runs in.\n",
    "\n",
    "***The Vertex AI Evaluation Component is a pipeline component that runs after the Vertex AI Batch Prediction Component.***\n",
    "\n",
    "\n",
    "### 3. The Pipeline\n",
    "Kubeflow pipeline components are factory functions that create pipeline steps. Each component describes the inputs, outputs, and implementation of the component.\n",
    "\n",
    "These components are linked together to create a reusable pipeline. This is done with the kfp.dsl package.\n",
    "\n",
    "***The Vertex AI Evaluation Component will be a step in the pipeline. This pipeline will be compiled then sent to MP to execute.***\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bwCyxycmQbmP",
    "ueJ1Oc54jUAx"
   ],
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
